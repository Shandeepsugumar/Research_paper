# Emotion Recognition App - Project Overview

## ğŸ¯ Project Summary

A professional Flutter mobile application implementing multimodal emotion recognition using Speech and Heart Rate signals with TensorFlow Lite models.

## âœ¨ Key Features

### 1. Speech Emotion Recognition (SER)
- Real-time audio recording through mobile microphone
- Advanced MFCC feature extraction (40 coefficients, 174 frames)
- 26 emotion class detection
- Confidence scores and probability distribution display

### 2. Heart Rate Emotion Recognition (HER)
- Google Fit API integration for seamless data retrieval
- Physiological signal processing with bandpass filtering
- Binary valence classification (Low/High)
- Heart rate statistics visualization

### 3. Multimodal Fusion Analysis
- Combined Speech + Heart Rate emotion detection
- Gated fusion neural network architecture
- 8 emotion classes: Angry, Calm, Disgust, Fear, Happy, Neutral, Sad, Surprise
- Step-by-step guided user workflow

## ğŸ—ï¸ Technical Architecture

### Technology Stack
- **Framework**: Flutter 3.0+
- **ML Inference**: TensorFlow Lite
- **Audio Processing**: Custom MFCC implementation
- **Health Data**: Google Fit API (health package)
- **State Management**: Provider pattern
- **Platform**: Android (with iOS support ready)

### Model Architecture

#### Speech Emotion Model (SER)
```
Input: [1, 40, 174] Float32
       â†“
Bidirectional RNN + Attention
       â†“
Output: [1, 26] Softmax probabilities
```

#### Heart Rate Model (HER)
```
Input: [1, 5000] Float32
       â†“
Conv1D + BiLSTM + Global Pooling
       â†“
Output: [1, 2] Softmax probabilities
```

#### Fusion Model
```
Input 1: [1, 40] Speech Features
Input 2: [1, 100, 1] HR Features
       â†“
Gated Fusion Network
       â†“
Output: [1, 8] Softmax probabilities
```

## ğŸ“ Project Structure

```
emotion_recognition_app/
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ main.dart                          # App entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ screens/                           # UI Screens
â”‚   â”‚   â”œâ”€â”€ home_screen.dart               # Main navigation
â”‚   â”‚   â”œâ”€â”€ speech_emotion_screen.dart     # SER interface
â”‚   â”‚   â”œâ”€â”€ heart_rate_screen.dart         # HER interface
â”‚   â”‚   â””â”€â”€ fusion_screen.dart             # Fusion interface
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                          # Business Logic
â”‚   â”‚   â”œâ”€â”€ speech_emotion_service.dart    # SER model inference
â”‚   â”‚   â”œâ”€â”€ heart_rate_service.dart        # HER model + Google Fit
â”‚   â”‚   â””â”€â”€ fusion_emotion_service.dart    # Fusion model inference
â”‚   â”‚
â”‚   â””â”€â”€ utils/                             # Utilities
â”‚       â””â”€â”€ audio_processor.dart           # MFCC computation
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ models/                            # TFLite Models
â”‚       â”œâ”€â”€ ser_cpu_rnn_model.tflite
â”‚       â”œâ”€â”€ HER_emotion_model_custom.tflite
â”‚       â””â”€â”€ fusion_model.tflite
â”‚
â”œâ”€â”€ android/                               # Android Configuration
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ build.gradle                   # App-level Gradle
â”‚   â”‚   â””â”€â”€ src/main/
â”‚   â”‚       â”œâ”€â”€ AndroidManifest.xml        # Permissions & config
â”‚   â”‚       â””â”€â”€ kotlin/...                 # MainActivity
â”‚   â”œâ”€â”€ build.gradle                       # Project-level Gradle
â”‚   â””â”€â”€ settings.gradle                    # Gradle settings
â”‚
â”œâ”€â”€ pubspec.yaml                           # Dependencies
â”œâ”€â”€ README.md                              # User documentation
â”œâ”€â”€ SETUP_INSTRUCTIONS.md                  # Setup guide
â””â”€â”€ PROJECT_OVERVIEW.md                    # This file
```

## ğŸ”¬ Signal Processing Pipeline

### Audio Processing (MFCC Extraction)

```
Raw Audio (22050 Hz, Mono WAV)
    â†“
Pre-emphasis Filter (Î±=0.97)
    â†“
Framing (512 samples, 256 hop)
    â†“
Hamming Windowing
    â†“
FFT (512 bins)
    â†“
Power Spectrum
    â†“
Mel Filterbank (40 bands)
    â†“
Log Transform
    â†“
DCT (40 coefficients)
    â†“
Padding/Truncation (174 frames)
    â†“
Model Input [1, 40, 174]
```

### Heart Rate Processing

```
Google Fit API Data
    â†“
Raw HR Values (BPM)
    â†“
Padding/Truncation (5000 samples)
    â†“
Z-score Normalization
    Î¼ = mean(HR)
    Ïƒ = std(HR)
    normalized = (HR - Î¼) / Ïƒ
    â†“
Model Input [1, 5000]
```

## ğŸ” Google Fit Integration

### Configuration Details
- **Package Name**: `com.emotion.app.emotion_recognition_app`
- **Client ID**: `522512286409-e1msqhisbo1ep47lqvug6b948i528pgp.apps.googleusercontent.com`
- **SHA-1**: `07:6E:8F:B1:3F:CE:E9:B2:79:DC:0F:EF:A8:35:84:F8:BB:EB:7A:9C`

### Required Permissions
```xml
<uses-permission android:name="android.permission.ACTIVITY_RECOGNITION"/>
<uses-permission android:name="android.permission.ACCESS_FINE_LOCATION"/>
<uses-permission android:name="com.google.android.gms.permission.ACTIVITY_RECOGNITION"/>
```

### Data Retrieval
- Fetches heart rate data from last 5 minutes
- Requires Google Fit app installed and synced
- Automatic permission handling

## ğŸ“± User Interface Design

### Color Scheme
- **Speech Module**: Blue gradient (#1976D2)
- **Heart Rate Module**: Red gradient (#D32F2F)
- **Fusion Module**: Green gradient (#388E3C)
- **Home Screen**: Teal gradient (#00897B)

### Design Principles
- Material Design 3 guidelines
- Professional gradient backgrounds
- Card-based layout for content
- Clear visual hierarchy
- Intuitive navigation flow
- Real-time status updates

## ğŸ”„ App Workflow

### Speech Emotion Recognition Flow
```
User opens SER screen
    â†“
Tap microphone button
    â†“
Grant microphone permission
    â†“
Record audio (user controlled)
    â†“
Tap stop button
    â†“
Extract MFCC features
    â†“
Run TFLite inference
    â†“
Display emotion + confidence
```

### Heart Rate Recognition Flow
```
User opens HER screen
    â†“
Tap "Fetch from Google Fit"
    â†“
Grant Fitness API permissions
    â†“
Retrieve HR data (5 min window)
    â†“
Preprocess HR signal
    â†“
Run TFLite inference
    â†“
Display valence + statistics
```

### Fusion Analysis Flow
```
User opens Fusion screen
    â†“
Step 1: Fetch HR data
    â†“
Step 2: Record voice
    â†“
Extract both features
    â†“
Run multimodal inference
    â†“
Display fused prediction
```

## ğŸ“Š Model Performance Expectations

### Input Requirements
- **Audio**: 3-5 seconds of clear speech
- **Heart Rate**: Continuous data from wearable device
- **Environment**: Quiet space for audio recording

### Latency
- **MFCC Extraction**: ~100-200ms
- **Model Inference**: ~50-100ms per model
- **Total Processing**: <500ms for complete pipeline

### Memory Usage
- **Model Size**: SER (~500KB), HER (~300KB), Fusion (~800KB)
- **Runtime Memory**: ~50-100MB including audio buffers
- **Peak Usage**: ~150MB during inference

## ğŸ› ï¸ Development Guidelines

### Code Organization
- **Single Responsibility**: Each file has one clear purpose
- **Separation of Concerns**: UI, business logic, and utilities separated
- **Clean Architecture**: Service layer abstracts model complexity
- **Reusable Components**: Audio processor shared across modules

### Best Practices
- Async/await for all I/O operations
- Proper error handling and user feedback
- Memory-efficient audio processing
- Model loaded once and reused
- State management with setState

## ğŸ” Key Implementation Details

### MFCC Computation
- Custom Dart implementation (no external DSP library)
- Cooley-Tukey FFT algorithm (radix-2)
- Mel-scale frequency warping
- DCT-II transformation
- Optimized for mobile performance

### Google Fit Integration
- OAuth 2.0 authentication flow
- Automatic token management
- Graceful permission handling
- Fallback error messages

### TFLite Integration
- Models loaded from assets
- Efficient buffer allocation
- Multi-input model support (fusion)
- Float32 tensor handling

## ğŸ“ Research Foundation

Based on the paper:
**"Enhanced Emotion Classification via Multimodal Fusion of Physiological and Vocal Signals from Daily-Life Wearables"**

### Scientific Contributions
1. Multimodal fusion architecture
2. Real-world wearable device integration
3. On-device inference for privacy
4. Practical emotion recognition system

### Emotion Classes

**SER Model (26 classes)**:
- Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised
- Each with variations: Strong, Normal, Weak intensity levels

**HER Model (2 classes)**:
- Low Valence (negative emotional state)
- High Valence (positive emotional state)

**Fusion Model (8 classes)**:
- Angry, Calm, Disgust, Fear, Happy, Neutral, Sad, Surprise

## ğŸ“ˆ Future Enhancements

### Planned Features
- [ ] Facial expression recognition module
- [ ] Continuous emotion tracking over time
- [ ] Emotion history and analytics dashboard
- [ ] Export emotion data (CSV/JSON)
- [ ] Personalized model fine-tuning
- [ ] Multiple language support

### Technical Improvements
- [ ] Model quantization for reduced size
- [ ] Real-time streaming inference
- [ ] Background heart rate monitoring
- [ ] Cloud sync for multi-device
- [ ] Advanced visualization (charts, graphs)

## ğŸ” Privacy & Security

### Data Handling
- **On-Device Processing**: All inference runs locally
- **No Cloud Upload**: Audio and HR data never leave device
- **Temporary Storage**: Recordings deleted after processing
- **Google Fit**: Only reads data, never writes
- **Permissions**: Requested only when needed

### Security Measures
- Secure model storage in assets
- No external API calls for inference
- Local file system isolation
- Permission-based access control

## ğŸ“ Testing Recommendations

### Unit Testing
- Audio processing functions
- MFCC computation accuracy
- Model input/output shapes
- Feature normalization

### Integration Testing
- Google Fit data retrieval
- Audio recording pipeline
- End-to-end inference flow
- UI state management

### User Testing
- Record various emotion expressions
- Test in different environments
- Verify with multiple users
- Compare with ground truth emotions

## ğŸ¯ Success Metrics

### Technical Metrics
- Model load time < 1 second
- Inference latency < 500ms
- App startup time < 2 seconds
- Memory usage < 200MB

### User Experience Metrics
- Clear emotion predictions
- Intuitive navigation flow
- Responsive UI interactions
- Helpful error messages

## ğŸ“š Documentation

### Available Guides
1. **README.md** - User-facing documentation
2. **SETUP_INSTRUCTIONS.md** - Detailed setup guide
3. **PROJECT_OVERVIEW.md** - This technical overview

### Code Documentation
- Inline comments for complex logic
- Function-level documentation
- Clear variable naming
- Structured file organization

## ğŸ¤ Contributing

### Code Style
- Follow Dart style guide
- Use meaningful variable names
- Add comments for complex algorithms
- Keep functions focused and small

### Git Workflow
1. Create feature branch
2. Implement changes
3. Test thoroughly
4. Submit pull request
5. Code review process

## ğŸ“ Support & Contact

For technical issues:
1. Check SETUP_INSTRUCTIONS.md
2. Review error messages carefully
3. Verify all prerequisites
4. Check model files are valid

## ğŸ† Acknowledgments

- TensorFlow Lite team for mobile inference
- Flutter team for cross-platform framework
- Google Fit API for health data access
- Research paper authors for scientific foundation

---

**Built with passion for emotion AI and mobile development** ğŸš€

*Version: 1.0.0*
*Last Updated: October 2025*
